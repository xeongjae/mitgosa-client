# MITGOSA(믿고사) - AI 리뷰 요약 서비스

<br />
<br />

<p align="center">
<img src="public/믿고사로고.png" alt="로고" width="700"/>
</p>

<p align="center">
  <strong>
  MITGOSA는 수많은 리뷰를 보며 많은 시간을 보내는 사람들을 위해, <br>원하는 상품의 전체 리뷰를 AI가 분석하여 한눈에 요약해주는 서비스입니다.
  </strong>
<br>
<a href="https://mitgosa.vercel.app">
    🔗 [https://mitgosa.vercel.app] 방문하기
</a>

<br />
<br />
<br />

# 🗂 목차

1. [💭 프로젝트 동기](#-프로젝트-동기)
2. [🛠 사용 기술](#-사용-기술)
3. [👀 미리보기 및 아키텍처](#-미리보기-및-아키텍처)
4. [🔥 주요 경험](#-주요-경험)
5. [💡 프로젝트 후기](#-프로젝트-후기)

<br />
<br />
<br />

# 💭 프로젝트 동기

이 프로젝트는 **인터넷 쇼핑을 좀 더 빠르고 편리하게** 돕고자 하는 생각에서 출발하였습니다.

온라인 마켓을 통해 쇼핑하며, 신뢰할 수 있는 정보를 얻기 위해 **수많은 리뷰를 하나하나 읽는 과정의 불편함**을 느꼈습니다. 저와 같은 사람들을 위해,<br /> 크롤링하여 얻은 리뷰 데이터를 **AI가 요약하고 정리**해줌으로써 사용자의 **시간과 노력을 획기적으로 절약**할 수 있을 것이라는 확신을 가지고 이 프로젝트를 시작하게 되었습니다.

저의 필요에서 비롯된 아이디어이기에 더 큰 애정을 가지고 개발에 임했으며, 그 속에 **기술적 도전을** 하나하나 달성하며 흥미를 느낄 수 있었습니다.

<br />
<br />
<br />

# 🛠 사용 기술

| 구분              | 기술                                                                                                                                                                                                                                                                                                                                                                                                            |
| ----------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Frontend**      | <img src="https://img.shields.io/badge/React-61DAFB?style=for-the-badge&logo=react&logoColor=black"> <img src="https://img.shields.io/badge/Vite-646CFF?style=for-the-badge&logo=vite&logoColor=white"> <img src="https://img.shields.io/badge/Sass-CC6699?style=for-the-badge&logo=sass&logoColor=white"> <img src="https://img.shields.io/badge/Axios-5A29E4?style=for-the-badge&logo=axios&logoColor=white"> |
| **Backend**       | <img src="https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=Node.js&logoColor=white"> <img src="https://img.shields.io/badge/Express-000000?style=for-the-badge&logo=express&logoColor=white">                                                                                                                                                                                               |
| **Crawling & AI** | <img src="https://img.shields.io/badge/Puppeteer-40B5A4?style=for-the-badge&logo=Puppeteer&logoColor=white"> <img src="https://img.shields.io/badge/Google_Gemini-4285F4?style=for-the-badge&logo=google&logoColor=white">                                                                                                                                                                                      |
| **Deployment**    | <img src="https://img.shields.io/badge/Vercel-000000?style=for-the-badge&logo=vercel&logoColor=white"> <img src="https://img.shields.io/badge/Render-46E3B7?style=for-the-badge&logo=render&logoColor=white">                                                                                                                                                                                                   |

<br />
<br />
<br />

  # 👀 미리보기 및 아키텍처
  
  <table>
    <tr align="center">
      <td>리뷰 분석</td>
      <td>UI / UX</td>
    </tr>
    <tr>
       <td width="50%">
        <img width="100%" src="public/user-gif.gif" />
      </td>
      <td width="50%">
        <img width="100%" src="public/ui-gif.gif" />
      </td>
    </tr>
  </table>

  ---
  
  <img src="public/architecture.png" alt="아키텍처" width="1000"/>

---

1.  **상품페이지 URL 수집:** 사용자가 상품 URL을 입력하면, 이를 서버로 전송합니다.
2.  **웹 크롤링:** 전달받은 URL을 `Puppeteer`를 이용해 크롤링하여 쇼핑몰의 리뷰 데이터를 수집합니다.
3.  **AI 리뷰 분석:** `Google Gemini API`를 활용하여 수집된 수백, 수천 개의 리뷰 텍스트를 분석하고 핵심적인 장점과 단점을 추출합니다.
4.  **Rest API 응답:** 분석된 결과(장점, 단점, 요약 등)를 클라이언트로 전달합니다.
5.  **분석 결과 시각화:** 분석된 결과를 바탕으로 상품의 장점, 단점, 종합 평점, 추천 대상까지 한눈에 보기 쉽게 제공합니다.

<br />

> **요약: 상품 URL 수집 >>>>>> 해당 상품의 리뷰 분석 결과 제공**

<br />
<br />
<br />

# 🔥 주요 경험

## 1. 크롤링을 사용한 필수 데이터 확보

> 크롤링(Crawling) = 웹 페이지의 데이터를 자동으로 가져오는 기술

이 프로젝트에서 중요한 데이터는 해당 상품의 리뷰 데이터와 상품 정보(상품명, 가격, 이미지 등) 입니다. <br />
때문에, 이 2가지 데이터를 가져오는 방법에 대한 고민을 가장 먼저 시작했고 크롤링을 사용해 데이터를 가져왔습니다.

---

### 1-1. 크롤링 사용 이유 : 오픈 API의 부재

웹 페이지 데이터를 활용하는 프로젝트 특성상 오픈 API 사용을 최우선으로 고려했습니다.<br />
그러나 무신사, 에이블리 등의 쇼핑몰 사이트는 오픈 API를 제공하지 않아 크롤링 도입이 필요하다고 판단했습니다.

오픈 API와 크롤링을 비교할 때 가장 중요한 차이점은 **데이터의 신뢰성**입니다. 공식적으로 제공되는 데이터가 아니기 때문에 신뢰성 검증이 필요했습니다.

> **본 프로젝트에서 신뢰성의 기준은 크롤링으로 수집한 리뷰 데이터의 개수가 실제 총 리뷰 개수와 얼마나 일치하는지로 설정했습니다.**


---

### 1-2. 가상 스크롤 크롤링 문제 : Puppeteer의 기능을 활용한 단계적 스크롤 로직 구현

초기에 크롤링 로직으로 리뷰를 수집한 결과, 10-20개 정도 소수의 리뷰만 수집되는 문제가 발생했습니다. <br />
개발자 도구를 분석한 결과, 초기에 예상했던 무한스크롤 방식이 아닌 **가상 스크롤(Virtual Scroll) 방식**으로 리뷰를 렌더링하고 있었습니다.

> **가상 스크롤: 사용자가 스크롤할 때마다 새로운 데이터를 동적으로 로드하는 방식.**

<img width="70%" src="public/virtual-s.png" />

무신사의 경우 순차적으로 한 번에 20-25개 정도의 리뷰만 렌더링됩니다. 이는 한 번의 크롤링으로 모든 리뷰를 수집할 수 없음을 의미했습니다.
<br /> 따라서 `page.waitForSelector()`와 `page.evaluate()`메서드를 활용하여 **단계적 스크롤 → 로딩 대기 → 데이터 수집**을 반복하였습니다.

이 후, 실제 리뷰 총 개수 대비 약 95-98%의 리뷰를 성공적으로 수집할 수 있었습니다.
<br />소량의 누락은 네트워크 지연이나 동적 로딩 타이밍 이슈로 인한 것으로 생각되었으나, 분석 속도와의 균형을 고려하여 만족할 만한 결과라고 판단했습니다.
 
다음은 위 내용을 토대로 진행한 테스트의 결과입니다.

<details>
<summary>[ 수집률 테스트 ]</summary>

<img width="70%" src="public/review-test.png" alt="리뷰 데이터 수집 정확도 테스트" />

</details>

---

### 1-3. 상품의 기본 정보 크롤링

리뷰 데이터 수집 이전에 상품 페이지에서 정보 데이터를 확보해야 했습니다.
<br />페이지의 DOM 구조를 분석한 결과, 각 상품 정보는 고유한 CSS Selector를 통해 스크래핑(Scraping) 할 수 있었습니다.

((((((<strong> 2-1. 게시글의 동영상 포함 여부 확인 </strong>

동영상을 얻기 위해서는 게시글에 동영상이 포함되어 있는지 확인하는 것이 첫 번째 단계라 생각했습니다. 이때, 모든 게시글 내부에 접근하여 포함 여부를 판단하는 것은 비효율적이라 판단했고, 내부에 접근하지 않고 동영상의 포함 여부를 확인하는 방법을 찾았습니다.

<img width="70%" src="https://github.com/user-attachments/assets/6ef2d471-097b-4d8f-b9ec-066faef714bc" />
</br>

해당 사진은 특정 카페의 전체 게시글 목록을 캡쳐한 사진입니다. 빨간색 동그라미 친 부분의 아이콘은 해당 게시글에 동영상이 포함되어 있음을 알려주는 아이콘입니다. 이 아이콘을 통해서 직접 게시글 내부에 접근하지 않아도 동영상의 포함 여부를 판단할 수 있게 되고, 해당 아이콘의 DOM 요소인 `span` 태그의 class 속성 `list-i-movie` 를 사용하여 전체 게시글 페이지만을 이동하며 동영상 포함 여부를 판단할 수 있었습니다.

---

<strong> 2-2. 예상하지 못한 크롤링 결과 데이터 </strong>

게시글 내부에 접근하지 않고, 동영상의 포함 여부를 확인하는 방법을 찾은 이후 크롤링 로직을 구현하여 결과를 확인했지만 예상하지 못한 결과를 확인했습니다.  
분명히 로그인을 완료하고, 컨텐츠의 로딩을 기다린 후 `page.content()` 메서드를 사용하여 본문 크롤링을 시도했지만, 기대했던 결과와 전혀 다른 결과가 반환됐습니다. 이유는 네이버 카페 게시글 페이지의 `iframe DOM` 구조 때문이었습니다.  
iframe은 부모 페이지와 독립적인 DOM 트리를 형성합니다. 다시 한번 개발자 도구를 확인하여 크롤링하고자 했던 DOM의 최상위 요소를 확인해 보니 iframe 태그로 감싸져 있는 걸 확인 할 수 있었습니다.

<img width="70%"  src="https://github.com/user-attachments/assets/542a9162-f2c5-4e86-aa60-189a9ca8f302" />

</br>

이를 해결 하기 위해 iframe 요소를 선택한 후, `contentFrame()` 메서드를 사용해 iframe 내부 `DOM 컨텍스트`를 확보하여 원하는 크롤링 데이터를 얻을 수 있게 되었습니다.)))))

---

<strong> 2-3. 게시글로부터 동영상 데이터 확보 </strong>

동영상이 포함된 게시글 주소를 확보한 이후, 동영상 데이터를 가져오기 위한 로직을 구현하기 시작했습니다.  
이를 위해 게시글 내부 video 태그의 `src` 속성을 확인했습니다.

<img width="70%"  src="https://github.com/user-attachments/assets/4234e6fb-da05-4114-b52b-aa77987d3b42" />

</br>

당연하게 존재할 것이라 생각한 video 태그의 src 속성은 존재하지 않았습니다. 해당 속성은 게시글 접근 초기에는 존재하지 않다가 동영상의 재생버튼을 클릭한 이후, 동적으로 생성되었습니다.

<img width="70%" src="https://github.com/user-attachments/assets/cb6827bd-c2e7-4dbb-822d-fa578d94df6c" />

</br>

이러한 구조라면 게시글에 접근하여 재생 버튼을 클릭한 다음, 다시 DOM 요소를 불러와야 하는 흐름이었고, 이 한 단계의 추가적인 행동이 동영상이 포함된 모든 게시글마다 실행된다면 크롤링 속도 저하에 영향을 끼친다고 판단했습니다.

해결하기 위한 방법을 찾던 중, 동영상 재생 시 스트리밍되는 데이터와 별개로 네트워크 요청에서 초기 데이터를 받아오는 경우가 있다는 정보를 얻었습니다. 실제로 네트워크 요청 탭을 확인해 보니, 수많은 요청 주소 중 해당 게시글의 동영상 데이터를 갖는 응답을 확인할 수 있었습니다.

<img width="70%" src="https://github.com/user-attachments/assets/2a775417-f6f9-4350-a490-173c3568b492" />

</br>

게시글 접속 시 발생하는 수많은 네트워크 요청 가운데, 동영상 데이터를 갖는 응답은 다른 요청들과는 다르게 유일한 주소 패턴을 갖고 있었습니다.

이러한 특징 덕분에 모든 네트워크 요청을 일일이 확인할 필요 없이 조건문 하나로 빠르게 필요한 네트워크 요청을 확인 할 수 있었고, 동영상 데이터를 가져올 수 있게 되었습니다.

---

## Gemini API를 사용한 리뷰 데이터 분석과 응답 데이터 관리

이 프로젝트에서는 필요한 데이터의 종류에 따라 서로 다른 API 요청을 서버에 전송합니다. 초기 개발 시에는 대부분의 기능을 메인페이지에서 구성하고, 몇 가지 모달을 사용한 UI를 구성했기에 전역 상태 관리를 사용하지 않은 로직을 작성했습니다. 그러나 프로젝트를 진행함에 따라 전역 상태 관리의 필요성을 느끼기 시작했습니다.

### 1. 길어지는 메인페이지의 코드 : 컴포넌트의 관심사 분리

대부분의 기능이 실행되는 메인페이지 코드의 길이는 프로젝트가 진행됨에 따라 점점 길어졌습니다. 늘어난 코드의 길이를 줄여 유지보수성을 향상하고 싶다는 생각, 메인 페이지는 동영상 재생 및 조작의 기능만 담당하는 관심사의 분리를 실현해 보고 싶다는 생각으로 API 요청 함수를 별도 파일로 분리하여 관리했습니다.  
하지만, 이 방식 역시 개선이 필요해 보였습니다.

<table>
  <div align="center">전역 상태 도입 전후 메인페이지 길이 비교</div>
  <tr>
    <td width="50%">
      <img  src="public/ui-gif.gif" />
    </td>
    <td width="50%">
      <img  src="public/user-gif.gif" />
    </td>
  </tr>
  <tr align="center">
    <td>로직 분리 전</td>
    <td>로직 분리 후</td>
  </tr>
</table>

> 분리 전후 약 100여줄 이상의 차이

---

<strong> 1-1. 다양한 API 요청 시점의 자동화 </strong>

다양한 API 요청 함수의 동작을 매번 명시적인 사용자 상호작용을 통하여 구현한다면 사용자 경험이 저하되고, 더하여 의도치 않은 중복 요청을 발생시킬 위험성이 존재할지도 모른다고 판단했습니다. 이러한 문제점을 해결하고자 API 요청을 상태 변화에 따라 자동으로 발생하도록 개선했습니다.  
다양한 API 요청 자동화의 트리거가 되는 값은 요청의 응답 데이터를 저장한 상태 값들이고, 해당 값들은 항상 일관된 상태를 갖기를 원했습니다. 이러한 개선 사항을 실현하기 위해 전역 상태 관리를 사용했습니다.

Zustand 전역 상태 관리 라이브러리를 도입한 후 개선된 사항은 다음과 같았습니다.

1. 응답 데이터로 받은 값을 전역 상태로 관리  
   이를 통하여 요청 자동화의 트리거가 되는 상태 값의 통일성과 신뢰성을 확보했습니다.
1. 전역 상태를 트리거로 한 API 요청의 자동화  
   다양한 시점에 필요한 API 요청의 자동화를 통하여, 사용자에게 별도의 동작을 요구하지 않고, 필요한 데이터를 반환받을 수 있게 되었습니다.

---

### 2. 전역 상태관리를 통한 에러처리 : 자동화 및 사용자 상호작용

전역 상태 관리를 사용한 이점에는 에러처리 방식도 포함이 됩니다. 해당 프로젝트에서 발생하는 대부분의 오류는 서버에서 올바른 크롤링 데이터를 반환받지 못할 때 발생합니다. 초기에는 서버단에서 발생하는 에러에 대하여 별다른 에러처리 방식이 없었습니다. 이러한 에러에 대하여 사용자에게 더욱더 나은 경험을 제공하기 위한 방법을 고민하였고, 다음과 같은 방식으로 에러처리 로직을 구현했습니다.

---

<strong> 2-1. 에러처리 방식 : 재요청 자동화 </strong>

사용자는 동영상을 시청 중이거나, 원하는 동영상을 검색하는 등 이미 프로젝트의 화면을 사용하고 있습니다. 이러한 상황에서 매번 에러가 발생할 때마다 명시적인 알람을 표시하는 처리 방식은 프로젝트와 적합하지 않다고 판단했습니다.  
이러한 프로젝트의 특수성에 알맞게 최대 2회의 에러에 대하여 자동으로 재요청 로직을 구성했습니다. 2회라는 기준은 일시적인 네트워크 장애 및 서버 응답 지연에 따라 발생할 수 있는 에러를 대비한 숫자입니다. 이러한 에러에 대하여 재요청을 통해 올바른 데이터를 확보할 수 있도록 구현했습니다.

---

<strong> 2-2. 에러처리 방식 : 선택권 부여 </strong>

2회의 자동 재요청 이후에도 발생하는 에러에 대하여 사용자에게 선택권을 제공했습니다. 발생하는 에러에 대하여 그 회수를 상태 값으로 카운트합니다. 3회 이상 발생 시, 에러에 대한 문구 및 버튼을 표시하는 모달을 제공하여 사용자로 하여금 재요청을 보낼지 해당 데이터 요청을 무시할지 선택할 수 있게 권한을 부여합니다.

이러한 에러처리 방식을 통하여 프로젝트 특성에 알맞게, 너무 잦은 UI 적 표현을 최소화하면서도 사용자 상호작용을 고려한 방식을 구현했습니다.

---

## 프로그레스바를 활용한 직관적 애니메이션 구현

초기 개발 단계에서는 동영상과 관련된 애니메이션이 없이 단순히 동영상만을 출력시켜 주는 UI 였습니다. 초기 목업 데이터를 통한 동영상 재생 시에는 불편함을 느끼지 못했지만, 실제 데이터를 통해 여러 카페의 여러 동영상을 다루다 보니 큰 불편함이 존재했습니다.  
카페별로 반환되는 동영상의 개수는 모두 다릅니다. 또, 영상을 시청하는 도중 새로운 동영상 데이터가 실시간으로 재생목록에 추가됩니다. 이때, "동영상이 총 몇 개인지?" , "현재 진 중인 동영상의 길이는 얼마나 긴지?" 와 관련된 정보를 직관적으로 보여준다면 사용하기 훨씬 편리해질 것 같다는 생각에 애니메이션을 구현하기 시작했습니다.

---

### 1. 초기 UI 구성 : 동영상의 총 개수 활용

프로그레스바에 총 동영상의 개수를 바탕으로 한 개별 동영상 개수를 표기했습니다.
배열을 활용하여 100%의 비율을 총 동영상의 개수로 나누었습니다. 이 값을 바탕으로 각 카페의 동영상 개수별로 또 추가되는 동영상의 개수를 반영한 UI 를 구성할 수 있었습니다.

배열 내 저장되어 있는 동영상의 위치를 이동할 경우 인덱스를 활용하여 이전의 동영상은 100% 진행률을 채운 상태로, 현재 및 이후의 동영상에 대해서는 0%의 진행률을 유지한 상태를 구현했습니다.

<img width="70%"  src="https://github.com/user-attachments/assets/f17451d2-b688-4326-82c1-e65e4e8cbb5a" />

---

### 2. 진행률 표현 : 차오르는 애니메이션 구현

현재 동영상의 진행률을 표현할 때, HTML5의 video 태그를 사용하여 프로그레스바가 차오르는 듯한 애니메이션을 다음과 같이 구현했습니다.

<strong> 2-1. useRef 와 video 태그를 활용한 재생 길이의 퍼센트 변환 </strong>

진행률을 실시간으로 표기하기 위해서는 영상의 재생 길이를 알아내야 했습니다. 동영상과 관련된 정보는 HTMLMediaElement의 속성을 사용하여 얻을 수 있습니다. 현재 영상 시간을 제공해 주는 `currentTime` 과 영상의 총길이를 제공해 주는 `duration` 속성을 활용했습니다. 해당 속성값들을 토대로 길이를 퍼센트로 변환하는 함수를 작성했습니다.
`(currentTime / duration) * 100` 이와 같은 간단한 수식을 통해서 현재 영상의 진행률을 얻어낼 수 있었습니다.

---

<strong> 2-2. timeupdate 이벤트를 활용한 애니메이션화 </strong>

동영상의 재생 상태 즉, 진행률이 변화할 때마다 프로그레스바를 자동으로 업데이트하기 위해 `timeupdate` 이벤트를 활용했습니다. 해당 이벤트는 동영상이 재생될 때마다 호출됩니다. 이를 통해 위에서 구한 진행률을 실시간으로 갱신시킬 수 있었습니다.  
이렇게 실시간으로 업데이트되는 진행률을 사용하여 차오르는 애니메이션을 구현할 수 있었습니다.

<table width="70%">
  <tr>
    <td>
      <img src="https://github.com/user-attachments/assets/f0fb0bb0-6c28-4443-95bf-abaec75815a1">
    </td>
  </tr>
  <tr align="center">
    <td>
      애니메이션화 UI
    </td>
  </tr>
</table>

<br />
<br />
<br />


# 💡 프로젝트 후기

- **비동기 처리와 에러 핸들링:** 크롤링과 외부 API 통신 과정에서 발생하는 다양한 비동기 작업을 효과적으로 제어하고, 예외 상황에 대한 안정적인 에러 처리 로직을 구축하며 Node.js의 비동기 처리 모델에 대한 깊은 이해를 얻을 수 있었습니다.
- **동적 웹 크롤링의 이해:** SPA(Single Page Application)로 구성된 최신 웹사이트의 구조를 분석하고, `Puppeteer`를 통해 JavaScript가 렌더링된 후의 최종 DOM에 접근하여 원하는 데이터를 정확히 수집하는 기술적 역량을 길렀습니다.
- **LLM(거대 언어 모델) 활용 능력:** `Gemini`와 같은 강력한 AI 모델을 활용하여 비정형 텍스트 데이터를 정제하고, 의미 있는 정보(장점, 단점, 요약)를 추출하는 방법을 학습했습니다. 단순히 API를 호출하는 것을 넘어, 원하는 결과물을 얻기 위한 효과적인 프롬프트 엔지니어링의 중요성을 깨달았습니다.

<br />
<br />
<br />
<br />
<br />
<br />
